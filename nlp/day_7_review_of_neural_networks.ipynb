{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 一、向量的内积 vs 矩阵的乘积 \n",
    "\n",
    "- 向量的内积(两个向量的相似度度量:np.dot): $\\boldsymbol{x} \\cdot \\boldsymbol{y} = x_1y_1 + x_2y_2 + \\cdots +x_ny_n$\n",
    "- 矩阵的乘积（权重矩阵对数据矩阵的线性变换:np.dot）：\n",
    "$$\n",
    "\\boldsymbol{C} = \\underbrace{\\boldsymbol{A}}_{数据样本矩阵}  \\times \\underbrace{\\boldsymbol{B}}_{权重矩阵}\n",
    "\n",
    "\\longrightarrow  \n",
    "\n",
    "\\boldsymbol{A}矩阵的行（样本）通过\\boldsymbol{B}矩阵的列（变换）进行处理。即\\boldsymbol{B}矩阵定义了如何对\\boldsymbol{A}矩阵的每个样本进行线性变换\n",
    "\n",
    "{\\color{Red} \\boldsymbol{A}与\\boldsymbol{B}交换顺序也行哈，本质还是运算的线性作用要分清！} \n",
    "$$\n",
    "- 矩阵的哈达玛积(逐元素积：A * B): 1对1帮扶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  二、神经网络的推理过程\n",
    "\n",
    "![](./attachements/神经网络的推理过程-神经元表示法.png)\n",
    "![](./attachements/神经网络的推理过程-层表示法.png)\n",
    "\n",
    "注意：\n",
    "\n",
    "- 所有的层都有 forward() 方法和 backward() 方法\n",
    "- 所有的层都有 params 和 grads 实例变量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 层-Affine\n",
    "\n",
    "class Affine:\n",
    "\n",
    "    def __init__(self, W, b) -> None:\n",
    "        self.params = [W, b]\n",
    "\n",
    "    def forward(self, x):\n",
    "        W,b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 层-Sigmoid\n",
    "\n",
    "class Sigmoid:\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.params = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.56104902  0.32854007]\n",
      " [ 1.27436108 -0.93733702]\n",
      " [ 1.27306454  0.01473727]\n",
      " [ 1.70195476 -0.55325669]\n",
      " [-0.10876628  1.32105778]\n",
      " [ 0.0033703  -0.09225441]\n",
      " [-0.18275277  1.03049557]\n",
      " [ 0.71801996  0.44650455]\n",
      " [-1.17649957  0.06730696]\n",
      " [ 0.66463137  1.73480992]]\n",
      " *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  * \n",
      "[[-0.20434097 -0.26609517  2.15629652]\n",
      " [-0.21039606 -0.25700322  1.80718516]\n",
      " [-0.09941655 -0.19731537  1.99742643]\n",
      " [-0.37184777 -0.35307236  2.02179563]\n",
      " [ 1.36129906  0.57095152  1.34675316]\n",
      " [ 1.12983928  0.46295758  1.15509034]\n",
      " [ 1.38957938  0.5914602   1.26418012]\n",
      " [ 0.42764283  0.08412406  1.7646761 ]\n",
      " [ 1.63048714  0.73735506  0.8925275 ]\n",
      " [ 0.59675533  0.1467331   1.95970569]]\n"
     ]
    }
   ],
   "source": [
    "# Neural Network - TwoLayerNet\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size) -> None:\n",
    "        \n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "\n",
    "        # 初始化权重和偏置\n",
    "        W1 = np.random.randn(I, H)\n",
    "        b1 = np.random.randn(H)\n",
    "        W2 = np.random.randn(H, O)\n",
    "        b2 = np.random.randn(O)\n",
    "\n",
    "\n",
    "        # 生成层\n",
    "        self.layers = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),    # 秒1\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "\n",
    "        # 将所有权重整理到列表中\n",
    "    \n",
    "\n",
    "    def predict(self, x):\n",
    "        # 实现前向传播\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x) # 秒2\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "x = np.random.randn(10, 2)\n",
    "model = TwoLayerNet(2, 4, 3)\n",
    "s = model.predict(x)\n",
    "print(x)\n",
    "print(\" * \" * 25)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 三、神经网络的学习\n",
    "\n",
    "- 损失函数: 神经网络当前学习到的参数好坏怎么度量？\n",
    "- 导数和梯度: 导数就是当自变量逼近时因变量的变化趋势，梯度是针对复合函数的，每一个自变量都有自己的导数！此为偏导数。\n",
    "- 链式法则：复合函数中引入的概念爷爷对孙子的导数 = 爷爷对爸爸的导数 x 爸爸对儿子的导数\n",
    "- 计算图：将输入数据 通过节点（操作）和 边（数据流）构建的图像结构，来表示运算的逻辑链路图\n",
    "- 反向传播：把神经网络看做是一个多层的复合函数，正向传播是逐层传递激活值，而反向传播是逐层传递损失函数对该层中节点的梯度，用来指导下一次正向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 矩阵的乘积实现\n",
    "\n",
    "![](./attachements/矩阵的乘积-正反向传播计算图.png)\n",
    "![](./attachements/矩阵的乘积-根据矩阵的形状推导反向传播式.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MatMul 矩阵乘法层实现\n",
    "\n",
    "class MatMul:\n",
    "\n",
    "    def __init__(self, W) -> None:\n",
    "        self.params = W\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dw = np.dot(self.x.T, dout)\n",
    "\n",
    "        self.grads[0][...] = dw # 深拷贝\n",
    "        return dx\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 带反向传播的Sigmoid层补全\n",
    "\n",
    "![](./attachements/Sigmoid层的反向传播计算图.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 带反向传播的Sigmoid层\n",
    "\n",
    "class Sigmoid:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 带反向传播的Affine层的实现\n",
    "\n",
    "![](./attachements/带反向传播的Affine层的实现.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 带反向传播的Affine层实现\n",
    "\n",
    "class Affine:\n",
    "\n",
    "    def __init__(self, W, b) -> None:\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4 Softmax with loss层的实现\n",
    "\n",
    "![](./attachements/Softmax-with-Loss层的计算图.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax with loss层的实现\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T\n",
    "\n",
    "    x = x - np.max(x)  # 溢出对策\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 监督数据是one-hot-vector的情况下，转换为正确解标签的索引\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None  # softmax的输出\n",
    "        self.t = None  # 监督数据\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size:  # 监督数据是one-hot-vector的情况\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5 SGD 俗称 Optimizer实现\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01) -> None:\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6 测试伪代码\n",
    "\n",
    "model = TwoLayerNet(2, 4, 3)\n",
    "optimizer = SGD()\n",
    "\n",
    "for i in  range(1000):\n",
    "    x_batch, t_batch = get_mini_batch(...) # 获取 mini-batch\n",
    "    loss = model.forward(x_batch, t_batch)\n",
    "    model.backward()\n",
    "    optimizer.update(model.params, model.grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 四、计算的高速化\n",
    "\n",
    "- 位精度：NumPy 的浮点数默认使用 64 位的数据类型。使用 32 位浮点数也可以无损地（识别精度几乎不下降）进行神经网络的推理和学习。从内存的角度来看，因为 32 位只有 64 位的一半，所以通常首选 32 位。\n",
    "\n",
    "- cupy:CuPy 和 NumPy 几乎拥有共同的 API，但是使用了GPU更快。不过我的mac是不行的，需要英伟达的GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp \n",
    "\n",
    "x = cp.arange(6).reshape(2, 3).astype('f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1NLP-1CODE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
