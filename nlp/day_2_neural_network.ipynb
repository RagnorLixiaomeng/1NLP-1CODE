{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一、神经网络\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1、神经网络是什么？\n",
    "\n",
    "神经网络就是具备：输出层、隐藏层、输出层的多神经元函数连接结构。本质是一个多元非线性函数拟合器！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2、神经网络的意义是什么？\n",
    "\n",
    "神经网络最大的意义就是：它可以自动地从数据中学习到合适的权重参数！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3、神经网络跟感知机有什么关系？\n",
    "\n",
    "- 单层感知机：激活函数使用了阶跃函数的模型！\n",
    "- 多层感知机：激活函数使用了非线性函数的模型！多层感知机又叫神经网络！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 二、激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1、激活函数激活的是什么？\n",
    "\n",
    "![激活函数的作用过程](./attachements/激活函数的作用过程.png)\n",
    "\n",
    "- 激活函数激活的是输入信号的总和，但更重要的是激活函数决定了如何来激活输入信号的总和！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2、为什么激活函数必须使用非线性函数？\n",
    "\n",
    "- 因为使用线性函数的话，加深神经网络的层数就没有意义了！\n",
    "- 没有意义指的是：不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”。\n",
    "\n",
    "$$\n",
    "\\underbrace{y(x) = h(h(h(x))) }_{{\\color{Red} 激活函数为linear的3层神经网络} } \n",
    "\n",
    "\\Longleftrightarrow \n",
    "\n",
    "y(x) = c \\times c \\times c \\times x \n",
    "\n",
    "\\Longleftrightarrow \n",
    "\n",
    "\\underbrace{y(x) = \\underbrace{a}_{a=c^3}  \\times x}_{{\\color{Red} 无隐藏层} } \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3、常用的激活函数\n",
    "\n",
    "###### 2.3.1、step\n",
    "\n",
    "> 阶跃函数如“竹筒敲石” \n",
    "\n",
    "$$\n",
    "h(x) = \\left\\{\\begin{matrix}\n",
    " 0 & (x \\le 0) \\\\ \n",
    " 1 & (x > 0)\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "```python\n",
    "# 阶跃函数定义\n",
    "y = np.array([0 if i <= 0 else 1 for i in x])\n",
    "```\n",
    "\n",
    "###### 2.3.2、sigmoid\n",
    "\n",
    "> sigmoid函数如“水车”\n",
    "\n",
    "$$\n",
    "h(x) = \\frac{1}{1 + exp(-x)} = \\frac{1}{1 + e^{-x}}  \n",
    "$$\n",
    "\n",
    "```python\n",
    "# sigmoid函数定义\n",
    "y = 1 / (1 + np.exp(-x))\n",
    "```\n",
    "\n",
    "###### 2.3.3、ReLU\n",
    "\n",
    "$$\n",
    "{\\color{Red} h(x) = \\left\\{\\begin{matrix}\n",
    " x & (x > 0) \\\\ \n",
    " 0 & (x \\le 0)\n",
    "\\end{matrix}\\right.} \n",
    "$$\n",
    "\n",
    "```python\n",
    "# ReLU函数定义\n",
    "y = np.maximum(0, x)\n",
    "```\n",
    "\n",
    "###### 2.3.4、softmax\n",
    "\n",
    "> 让我看看谁最富\n",
    "\n",
    "对于一个包含 $n$ 个元素的输入向量 $\\mathbf{z} = [z_1, z_2, \\dots, z_n]$，Softmax的输出为：\n",
    "\n",
    "$$\n",
    "\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}, \\quad i = 1, 2, \\dots, n\n",
    "$$\n",
    "\n",
    "这意味着 Softmax 会将每个元素转化为一个正数，且所有元素之和为1。\n",
    "\n",
    "```python\n",
    "# softmax函数定义,注意这个实现会溢出，并不真正使用\n",
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "\n",
    "    return y\n",
    "```\n",
    "\n",
    "###### 2.3.5、softmax函数的数值溢出问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1010, 1000, 990])\n",
    "np.exp(a) / np.sum(np.exp(a)) # array([nan, nan, nan]) 没有被正确计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.max(a)\n",
    "a - c\n",
    "np.exp(a - c ) / np.sum(np.exp(a - c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的道理在这：\n",
    "\n",
    "$$\n",
    "{\\color{Green} \\begin{aligned}\n",
    "y_{k}=\\frac{\\exp \\left(a_{k}\\right)}{\\sum_{i=1}^{n} \\exp \\left(a_{i}\\right)} & =\\frac{\\mathrm{Cexp}\\left(a_{k}\\right)}{\\mathrm{C} \\sum_{i=1}^{n} \\exp \\left(a_{i}\\right)} \\\\\n",
    "& =\\frac{\\exp \\left(a_{k}+\\log \\mathrm{C}\\right)}{\\sum_{i=1}^{n} \\exp \\left(a_{i}+\\log \\mathrm{C}\\right)} \\\\\n",
    "& =\\frac{\\exp \\left(a_{k}+\\mathrm{C}^{\\prime}\\right)}{\\sum_{i=1}^{n} \\exp \\left(a_{i}+\\mathrm{C}^{\\prime}\\right)}\n",
    "\\end{aligned}} \n",
    "$$\n",
    "\n",
    "说明，在进行 softmax 的指数函数的运算时，加上（或者减去）某个常数并不会改变运算的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 阶跃函数绘制\n",
    "x = np.linspace(-10, 10, 400)\n",
    "y = np.array([0 if i <= 0 else 1 for i in x])\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(x, y, color='b', label='Step Function')\n",
    "plt.title('Step Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('h(x)')\n",
    "plt.axhline(0, color='black', lw=0.5)\n",
    "plt.axvline(0, color='black', lw=0.5)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 2. Sigmoid函数绘制\n",
    "x = np.linspace(-10, 10, 400)\n",
    "y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(x, y, color='g', label='Sigmoid Function')\n",
    "plt.title('Sigmoid Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('σ(x)')\n",
    "plt.axhline(0, color='black', lw=0.5)\n",
    "plt.axvline(0, color='black', lw=0.5)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 3. ReLU函数绘制\n",
    "x = np.linspace(-10, 10, 400)\n",
    "y = np.maximum(0, x)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(x, y, color='r', label='ReLU Function')\n",
    "plt.title('ReLU Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('ReLU(x)')\n",
    "plt.axhline(0, color='black', lw=0.5)\n",
    "plt.axvline(0, color='black', lw=0.5)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 4. Softmax函数绘制\n",
    "x_values = np.array([i for i in range(-2, 3)])  # 输入值为离散的几个点\n",
    "\n",
    "def softmax(x_values):\n",
    "    exp_values = np.exp(x_values - np.max(x_values))  # 减去最大值以提高数值稳定性\n",
    "    return exp_values / np.sum(exp_values)\n",
    "\n",
    "y_values = softmax(x_values)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(x_values, y_values, color='m', label='Softmax Function')\n",
    "plt.title('Softmax Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Softmax(x)')\n",
    "plt.axhline(0, color='black', lw=0.5)\n",
    "plt.axvline(0, color='black', lw=0.5)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 三、3层神经网络\n",
    "\n",
    "##### 3.1.1、层神经网络中信号是如何传递的？\n",
    "\n",
    "- Step-1 权重符号的定义\n",
    "![权重符号的定义](./attachements/singnal_pass_through_nn_权重符号.png)\n",
    "\n",
    "\n",
    "- Step-2 明确表示出偏置\n",
    "\n",
    "$y = h(b +w_1x_1 + w_2x_2)$\n",
    "\n",
    "![明确表示出偏置](./attachements/singnal_pass_through_nn_明确表示出偏置.png)\n",
    "\n",
    "\n",
    "- Step-3 输入层到第一层的信号传递\n",
    "![输入层到第一层的信号传递](./attachements/singnal_pass_through_nn_输入层到第1层的信号传递.png)\n",
    "\n",
    "$$\n",
    "a^{(1)}_1 = w^{(1)}_{11}x_1 + w^{(1)}_{12}x_2 + b^{(1)}_1\n",
    "$$\n",
    "\n",
    "写成矩阵的乘法就是$\\boldsymbol{A}^{(1)} = \\boldsymbol{X} \\boldsymbol{W}^{(1)} + \\boldsymbol{B}^{(1)}$\n",
    "\n",
    "其中\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{A}^{(1)} & =\\left(\\begin{array}{lll}\n",
    "a_{1}^{(1)} & a_{2}^{(1)} & a_{3}^{(1)}\n",
    "\\end{array}\\right), \\boldsymbol{X}=\\left(\\begin{array}{ll}\n",
    "x_{1} & x_{2}\n",
    "\\end{array}\\right), \\boldsymbol{B}^{(1)}=\\left(\\begin{array}{lll}\n",
    "b_{1}^{(1)} & b_{2}^{(1)} & b_{3}^{(1)}\n",
    "\\end{array}\\right) \\\\\n",
    "\\boldsymbol{W}^{(1)} & =\\left(\\begin{array}{lll}\n",
    "w_{11}^{(1)} & w_{21}^{(1)} & w_{31}^{(1)} \\\\\n",
    "w_{12}^{(1)} & w_{22}^{(1)} & w_{32}^{(1)}\n",
    "\\end{array}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "- Step-4 第一层中信号被激活函数激活过程\n",
    "\n",
    "![第一层中信号被激活函数激活过程](./attachements/singnal_pass_through_nn_第一层输出到激活函数激活.png)\n",
    "\n",
    "\n",
    "\n",
    "- Step-05 第1层到第2层的信号传递\n",
    "\n",
    "![第1层到第2层的信号传递](./attachements/singnal_pass_through_nn_第1层到第2层的信号传递.png)\n",
    "\n",
    "\n",
    "- Step-06 第2层到输出层的信号传递\n",
    "\n",
    "![第2层到输出层的信号传递](./attachements/singnal_pass_through_nn_第2层到输出层的信号传递.png)\n",
    "\n",
    "\n",
    "\n",
    "##### 3.1.2、如何利用矩阵乘法实现神经网络的前向处理？\n",
    "\n",
    "所谓利用矩阵乘法其实就是把参数($w$、$b$)抽出来放在矩阵中，而矩阵乘法可以表示多元线性方程组罢了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def identity_function(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def init_network():\n",
    "    network = {}\n",
    "    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "    network['b1'] = np.array([0.1, 0.2, 0.3])\n",
    "    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "    network['b2'] = np.array([0.1, 0.2])\n",
    "    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "    network['b3'] = np.array([0.1, 0.2])\n",
    "    return network\n",
    "\n",
    "\n",
    "def forward(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = identity_function(a3)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "network = init_network()\n",
    "x = np.array([1.0, 0.5])\n",
    "y = forward(network, x)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 代码解析\n",
    "\n",
    "这里面最迷的就是参数的维度定义，其实就是2个点：\n",
    "\n",
    "- $W$: 比如$W1$是输入到第1层的权重矩阵，那输入层有2个节点$x_1$和$x_2$,而第1层隐藏层有3个神经元,因此权重矩阵的维度就是$2 \\times 3$,即输入层的每个节点都连接到第1层的每个神经元！\n",
    "\n",
    "- $b$：比如$b1$是第1层隐藏层的偏置项，而第1层有3个神经元，每个神经元都有自己的一个偏置，因此偏置向量的维度就是$1 \\times 3$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.3、如何设计输出层？\n",
    "\n",
    "其实输出层也就是神经元，本质2个点\n",
    "\n",
    "- 选激活函数：回归问题用恒等函数；分类问题用softmax函数。\n",
    "- 定输出层的神经元数量：根据待解决的问题来定，比如分类问题一般设定为类别数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 四、自己从0实现一个NN完成MNIST手写数字识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded network parameters from sample_weight.pkl\n",
      "Processed batch 1: Accuracy so far = 0.1100\n",
      "Processed batch 2: Accuracy so far = 0.1000\n",
      "Processed batch 3: Accuracy so far = 0.0833\n",
      "Processed batch 4: Accuracy so far = 0.1025\n",
      "Processed batch 5: Accuracy so far = 0.1060\n",
      "Processed batch 6: Accuracy so far = 0.0983\n",
      "Processed batch 7: Accuracy so far = 0.0986\n",
      "Processed batch 8: Accuracy so far = 0.0975\n",
      "Processed batch 9: Accuracy so far = 0.0956\n",
      "Processed batch 10: Accuracy so far = 0.0950\n",
      "Processed batch 11: Accuracy so far = 0.0955\n",
      "Processed batch 12: Accuracy so far = 0.0975\n",
      "Processed batch 13: Accuracy so far = 0.0962\n",
      "Processed batch 14: Accuracy so far = 0.0993\n",
      "Processed batch 15: Accuracy so far = 0.0973\n",
      "Processed batch 16: Accuracy so far = 0.0950\n",
      "Processed batch 17: Accuracy so far = 0.0935\n",
      "Processed batch 18: Accuracy so far = 0.0922\n",
      "Processed batch 19: Accuracy so far = 0.0921\n",
      "Processed batch 20: Accuracy so far = 0.0945\n",
      "Processed batch 21: Accuracy so far = 0.0971\n",
      "Processed batch 22: Accuracy so far = 0.0995\n",
      "Processed batch 23: Accuracy so far = 0.1009\n",
      "Processed batch 24: Accuracy so far = 0.1013\n",
      "Processed batch 25: Accuracy so far = 0.1004\n",
      "Processed batch 26: Accuracy so far = 0.1012\n",
      "Processed batch 27: Accuracy so far = 0.1015\n",
      "Processed batch 28: Accuracy so far = 0.1014\n",
      "Processed batch 29: Accuracy so far = 0.1017\n",
      "Processed batch 30: Accuracy so far = 0.1017\n",
      "Processed batch 31: Accuracy so far = 0.0990\n",
      "Processed batch 32: Accuracy so far = 0.0991\n",
      "Processed batch 33: Accuracy so far = 0.0988\n",
      "Processed batch 34: Accuracy so far = 0.0991\n",
      "Processed batch 35: Accuracy so far = 0.0989\n",
      "Processed batch 36: Accuracy so far = 0.0994\n",
      "Processed batch 37: Accuracy so far = 0.0986\n",
      "Processed batch 38: Accuracy so far = 0.0997\n",
      "Processed batch 39: Accuracy so far = 0.1005\n",
      "Processed batch 40: Accuracy so far = 0.1000\n",
      "Processed batch 41: Accuracy so far = 0.1002\n",
      "Processed batch 42: Accuracy so far = 0.0998\n",
      "Processed batch 43: Accuracy so far = 0.0991\n",
      "Processed batch 44: Accuracy so far = 0.0984\n",
      "Processed batch 45: Accuracy so far = 0.0993\n",
      "Processed batch 46: Accuracy so far = 0.0991\n",
      "Processed batch 47: Accuracy so far = 0.0985\n",
      "Processed batch 48: Accuracy so far = 0.0988\n",
      "Processed batch 49: Accuracy so far = 0.0982\n",
      "Processed batch 50: Accuracy so far = 0.0980\n",
      "Processed batch 51: Accuracy so far = 0.0969\n",
      "Processed batch 52: Accuracy so far = 0.0967\n",
      "Processed batch 53: Accuracy so far = 0.0962\n",
      "Processed batch 54: Accuracy so far = 0.0967\n",
      "Processed batch 55: Accuracy so far = 0.0965\n",
      "Processed batch 56: Accuracy so far = 0.0961\n",
      "Processed batch 57: Accuracy so far = 0.0968\n",
      "Processed batch 58: Accuracy so far = 0.0976\n",
      "Processed batch 59: Accuracy so far = 0.0971\n",
      "Processed batch 60: Accuracy so far = 0.0972\n",
      "Processed batch 61: Accuracy so far = 0.0962\n",
      "Processed batch 62: Accuracy so far = 0.0963\n",
      "Processed batch 63: Accuracy so far = 0.0959\n",
      "Processed batch 64: Accuracy so far = 0.0956\n",
      "Processed batch 65: Accuracy so far = 0.0958\n",
      "Processed batch 66: Accuracy so far = 0.0962\n",
      "Processed batch 67: Accuracy so far = 0.0969\n",
      "Processed batch 68: Accuracy so far = 0.0976\n",
      "Processed batch 69: Accuracy so far = 0.0981\n",
      "Processed batch 70: Accuracy so far = 0.0976\n",
      "Processed batch 71: Accuracy so far = 0.0979\n",
      "Processed batch 72: Accuracy so far = 0.0979\n",
      "Processed batch 73: Accuracy so far = 0.0978\n",
      "Processed batch 74: Accuracy so far = 0.0980\n",
      "Processed batch 75: Accuracy so far = 0.0980\n",
      "Processed batch 76: Accuracy so far = 0.0976\n",
      "Processed batch 77: Accuracy so far = 0.0978\n",
      "Processed batch 78: Accuracy so far = 0.0981\n",
      "Processed batch 79: Accuracy so far = 0.0975\n",
      "Processed batch 80: Accuracy so far = 0.0983\n",
      "Processed batch 81: Accuracy so far = 0.0980\n",
      "Processed batch 82: Accuracy so far = 0.0977\n",
      "Processed batch 83: Accuracy so far = 0.0977\n",
      "Processed batch 84: Accuracy so far = 0.0981\n",
      "Processed batch 85: Accuracy so far = 0.0981\n",
      "Processed batch 86: Accuracy so far = 0.0985\n",
      "Processed batch 87: Accuracy so far = 0.0985\n",
      "Processed batch 88: Accuracy so far = 0.0983\n",
      "Processed batch 89: Accuracy so far = 0.0978\n",
      "Processed batch 90: Accuracy so far = 0.0980\n",
      "Processed batch 91: Accuracy so far = 0.0975\n",
      "Processed batch 92: Accuracy so far = 0.0975\n",
      "Processed batch 93: Accuracy so far = 0.0972\n",
      "Processed batch 94: Accuracy so far = 0.0974\n",
      "Processed batch 95: Accuracy so far = 0.0975\n",
      "Processed batch 96: Accuracy so far = 0.0982\n",
      "Processed batch 97: Accuracy so far = 0.0986\n",
      "Processed batch 98: Accuracy so far = 0.0987\n",
      "Processed batch 99: Accuracy so far = 0.0986\n",
      "Processed batch 100: Accuracy so far = 0.0983\n",
      "Processed batch 101: Accuracy so far = 0.0979\n",
      "Processed batch 102: Accuracy so far = 0.0985\n",
      "Processed batch 103: Accuracy so far = 0.0988\n",
      "Processed batch 104: Accuracy so far = 0.0988\n",
      "Processed batch 105: Accuracy so far = 0.0986\n",
      "Processed batch 106: Accuracy so far = 0.0988\n",
      "Processed batch 107: Accuracy so far = 0.0985\n",
      "Processed batch 108: Accuracy so far = 0.0981\n",
      "Processed batch 109: Accuracy so far = 0.0978\n",
      "Processed batch 110: Accuracy so far = 0.0979\n",
      "Processed batch 111: Accuracy so far = 0.0981\n",
      "Processed batch 112: Accuracy so far = 0.0982\n",
      "Processed batch 113: Accuracy so far = 0.0984\n",
      "Processed batch 114: Accuracy so far = 0.0984\n",
      "Processed batch 115: Accuracy so far = 0.0981\n",
      "Processed batch 116: Accuracy so far = 0.0977\n",
      "Processed batch 117: Accuracy so far = 0.0976\n",
      "Processed batch 118: Accuracy so far = 0.0976\n",
      "Processed batch 119: Accuracy so far = 0.0982\n",
      "Processed batch 120: Accuracy so far = 0.0979\n",
      "Processed batch 121: Accuracy so far = 0.0980\n",
      "Processed batch 122: Accuracy so far = 0.0981\n",
      "Processed batch 123: Accuracy so far = 0.0980\n",
      "Processed batch 124: Accuracy so far = 0.0978\n",
      "Processed batch 125: Accuracy so far = 0.0978\n",
      "Processed batch 126: Accuracy so far = 0.0976\n",
      "Processed batch 127: Accuracy so far = 0.0974\n",
      "Processed batch 128: Accuracy so far = 0.0974\n",
      "Processed batch 129: Accuracy so far = 0.0974\n",
      "Processed batch 130: Accuracy so far = 0.0973\n",
      "Processed batch 131: Accuracy so far = 0.0973\n",
      "Processed batch 132: Accuracy so far = 0.0973\n",
      "Processed batch 133: Accuracy so far = 0.0972\n",
      "Processed batch 134: Accuracy so far = 0.0972\n",
      "Processed batch 135: Accuracy so far = 0.0971\n",
      "Processed batch 136: Accuracy so far = 0.0969\n",
      "Processed batch 137: Accuracy so far = 0.0970\n",
      "Processed batch 138: Accuracy so far = 0.0968\n",
      "Processed batch 139: Accuracy so far = 0.0966\n",
      "Processed batch 140: Accuracy so far = 0.0959\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "\n",
    "def get_data():\n",
    "    # 使用 sklearn 的内置函数加载 MNIST 数据集\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "    x = mnist.data / 255.0  # 归一化处理\n",
    "    t = mnist.target.astype(int)\n",
    "    x_train, x_test, t_train, t_test = train_test_split(x, t, test_size=0.2, random_state=42)\n",
    "    return x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    # 如果已经存在权重文件，则加载\n",
    "    if os.path.exists(\"sample_weight.pkl\"):\n",
    "        with open(\"sample_weight.pkl\", 'rb') as f:\n",
    "            network = pickle.load(f)\n",
    "        print(\"Loaded network parameters from sample_weight.pkl\")\n",
    "    else:\n",
    "        # 初始化网络参数\n",
    "        network = {}\n",
    "        input_size = 784  # 输入层大小（MNIST图像是28x28像素）\n",
    "        hidden_layer1_size = 50  # 隐藏层1大小\n",
    "        hidden_layer2_size = 50  # 隐藏层2大小\n",
    "        output_size = 10  # 输出层大小（10个分类）\n",
    "\n",
    "        # 使用随机数初始化权重和偏置\n",
    "        np.random.seed(42)\n",
    "        network['W1'] = np.random.randn(input_size, hidden_layer1_size) * 0.01\n",
    "        network['b1'] = np.zeros(hidden_layer1_size)\n",
    "        network['W2'] = np.random.randn(hidden_layer1_size, hidden_layer2_size) * 0.01\n",
    "        network['b2'] = np.zeros(hidden_layer2_size)\n",
    "        network['W3'] = np.random.randn(hidden_layer2_size, output_size) * 0.01\n",
    "        network['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 保存初始化的网络参数\n",
    "        with open(\"sample_weight.pkl\", 'wb') as f:\n",
    "            pickle.dump(network, f)\n",
    "        print(\"Initialized and saved network parameters to sample_weight.pkl\")\n",
    "\n",
    "    return network\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    c = np.max(x)\n",
    "    exp_x = np.exp(x - c)  # 数值稳定性\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "def predict(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "    # 前向传播\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "    return y\n",
    "\n",
    "x, t = get_data()\n",
    "network = init_network()\n",
    "accuracy_cnt = 0\n",
    "\n",
    "# 单张图片识别\n",
    "# for i in range(len(x)):\n",
    "#     y = predict(network, x[i])\n",
    "#     p = np.argmax(y)  # 获取概率最高的元素的索引\n",
    "#     if p == t[i]:\n",
    "#         accuracy_cnt += 1\n",
    "\n",
    "# print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))\n",
    "\n",
    "# 批处理每次100张\n",
    "batch_size = 100  # 每次处理的批大小\n",
    "\n",
    "for i in range(0, len(x), batch_size):\n",
    "    x_batch = x[i:i+batch_size]\n",
    "    y_batch = predict(network, x_batch)\n",
    "    p = np.argmax(y_batch, axis=1)  # 获取每行中概率最高的元素的索引\n",
    "    accuracy_cnt += np.sum(p == t[i:i+batch_size])\n",
    "    print(f\"Processed batch {i // batch_size + 1}: Accuracy so far = {float(accuracy_cnt) / (i + len(x_batch)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1、为什么随机初始化的准确率会在 10% 左右？\n",
    "\n",
    "1. **MNIST 数据集有 10 个类别**\n",
    "   - MNIST 数据集是一个手写数字数据集，包含从 0 到 9 共 10 个不同的类别。\n",
    "   - 如果我们用一个完全随机的方式来进行分类（即没有学习的网络），理论上每个数字被随机分类的概率是相同的，因此分类的准确率大约是 \\( \\frac{1}{10} = 0.1 \\) 或 10%。\n",
    "\n",
    "2. **当前的网络随机权重**\n",
    "   - 在当前代码中，网络的权重是使用随机数初始化的，且偏置为零。这意味着在预测的时候，输出是没有任何训练过程的权重直接应用于输入，得到了一个随机的结果。\n",
    "   - 对于 10 个类别的随机选择，理论上准确率会在 10% 左右。因此，9.6% 是非常合理的表现，符合随机猜测的准确率。\n",
    "\n",
    "##### 4.2、怎么提高准确率？\n",
    "\n",
    "为了让这个神经网络学习并提高其预测准确率，您需要进行训练。这包括：\n",
    "1. **使用反向传播算法**来调整权重和偏置，使得网络能够更好地拟合数据。\n",
    "2. **定义损失函数**（如交叉熵损失），并通过优化算法（如梯度下降）来最小化损失函数。\n",
    "3. 通过多轮次的迭代训练，不断更新网络的参数，使得模型能够逐步学习如何将输入的特征映射到正确的输出类别。\n",
    "\n",
    "\n",
    "##### 4.3、批处理\n",
    "\n",
    "- 输入数据的集合称为批，批处理的本质就是“打包输入数据”。通过以批为单位进行推理处理，能够实现高速的运算（因为大多数数值计算的库都对大型数组运算做了优化！）。\n",
    "\n",
    "![没有使用批处理的数组运算流](./attachements/没有使用批处理的数组运算流.png)\n",
    "![使用了批处理的数组运算流](./attachements/使用了批处理的数组运算流.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100  # 每次处理的批大小\n",
    "\n",
    "for i in range(0, len(x), batch_size):\n",
    "    x_batch = x[i:i+batch_size]\n",
    "    y_batch = predict(network, x_batch)\n",
    "    p = np.argmax(y_batch, axis=1)  # 获取每行中概率最高的元素的索引\n",
    "    accuracy_cnt += np.sum(p == t[i:i+batch_size])\n",
    "    print(f\"Processed batch {i // batch_size + 1}: Accuracy so far = {float(accuracy_cnt) / (i + len(x_batch)):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1NLP-1CODE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
