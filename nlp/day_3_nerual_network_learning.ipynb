{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一、神经网络是怎么学习的？\n",
    "\n",
    "###### 背景：感知机、多层感知机（神经网络）中的$w$跟$b$都是人确定的,层数更深的nn上亿参数，你怎么手工设？工作量太大了！\n",
    "###### 学什么: 所谓“学习”就是从训练数据中自动获取最优权重参数的过程。\n",
    "###### 怎么学（学习步骤）：\n",
    "- 步骤1：mini-batch\n",
    "从训练数据中随机选出一部分数据，这部分数据称为mini-batch，目标就是减小mini-batch的损失函数的值。\n",
    "- 步骤2：计算梯度\n",
    "为了减少mini-batch的损失函数的值，需要求出各个权重参数的梯度，梯度表示损失函数的值减少最多的方向。\n",
    "- 步骤3：更新参数\n",
    "将权重参数沿梯度方向进行微小更新。\n",
    "- 步骤4：重复\n",
    "重复步骤1、2、3\n",
    "\n",
    "###### 学到什么程度：获得泛化能力是机器学习的最终目标！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 二、为什么数据是机器学习的命根子？\n",
    "\n",
    "###### 人工编程 vs 机器学习 vs 神经网络：![](./attachements/传统编程vs机器学习vs深度学习.png)\n",
    "\n",
    "###### 什么是特征量？怎么选取合适的特征量？\n",
    "    - 所谓特征量，指的是可以从输入数据中准确提取本质数据的转换器。比如在手写数字识别中，原始数据是像素组成的图片，而特征量可以是边缘、角点、形状等信息，它们能很好地描述这个数字的特点，从而帮助模型更准确地判断这是什么数字。\n",
    "\n",
    "###### 为什么说深度学习是end-to-end machine learning？\n",
    "    - 因为它不考虑特征量。或者说隐层自己“发现”特征量。比如不管是人脸识别还是狗脸识别神经网络都是通过不断地学习所提供的数据，尝试发现待求解问题的模式。NN的求解模板是通用的！\n",
    "\n",
    "###### 为什么一般把数据划分为Train跟Test？\n",
    "    - 因为模型的终极目标是泛化能力强！在训练数据中学习找到的最优参数必须要在测试数据中评估下，因为测试数据模型没见过代表了一定的泛化能力！\n",
    "\n",
    "###### 机器学习的最终目标是什么？\n",
    "    - 泛化能力强！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 三、损失函数\n",
    "\n",
    "###### 损失函数是损失什么的函数？\n",
    "    \n",
    "    - 所有的损失函数目的只有一个：衡量当前的神经网络模型（参数）对监督数据在多大程度上不拟合（不一致）！\n",
    "\n",
    "\n",
    "###### 均方误差MSE\n",
    "\n",
    "    - 公式：$\\text{MSE} = \\frac{1}{\\underbrace{N}_{样本的数量}} \\sum_{i=1}^{N} (\\underbrace{y_i}_{第i个样本的真实值}  - \\underbrace{\\hat{y}_i}_{第i个样本的预测值})^2$\n",
    "    - 总结：MSE的名字已经很明显了，均、方。它的本质是衡量每一个真实与预测的差距的平均，肯定差距越小越好，也就是MSE越小越好！\n",
    "\n",
    "###### 交叉熵误差\n",
    "    > One-hot编码：衡量的是模型对正确类别的预测有多好！ vs 非One-hot编码：衡量模型对真实类别的预测概率有多高！交叉熵误差越小越好，但接近于 0 而不是完全为 0\n",
    "\n",
    "    - One-hot 编码形式的交叉熵误差：$\\text{Cross Entropy Error} = - \\frac{1}{\\underbrace{N}_{\\text{样本的数量}}} \\sum_{i=1}^{N} \\sum_{j=1}^{C} \\underbrace{t_{ij}}_{\\text{第} i \\text{个样本的真实标签 (one-hot 编码)}} \\cdot \\log(\\underbrace{y_{ij}}_{\\text{模型对第} i \\text{个样本的第} j \\text{类的预测概率}} + \\epsilon)$\n",
    "\n",
    "    - 非 One-hot 编码形式（使用索引表示真实标签）的交叉熵误差: $\\text{Cross Entropy Error} = -\\frac{1}{\\underbrace{N}_{\\text{样本的数量}}} \\sum_{i=1}^{N} \\log(\\underbrace{y_{i, t_i}}_{\\text{第} i \\text{个样本中真实类别的预测概率}} + \\epsilon)$\n",
    "\n",
    "    - $N$：样本的数量。\n",
    "    - $C$：类别的数量（在 one-hot 编码形式中）。\n",
    "    - $t_{ij}$：第 $i$ 个样本的真实标签（如果是 one-hot 编码，只有对应的类别为 1，其余为 0）。\n",
    "    - $y_{ij}$：模型对第 $i$ 个样本属于第 $j$ 类的预测概率。\n",
    "    - $t_i$：第 $i$ 个样本的真实类别索引（非 one-hot 表示）。\n",
    "    - $y_{i, t_i}$：第 $i$ 个样本中真实类别的预测概率。\n",
    "    - $\\epsilon$：为了防止 $\\log(0)$ 的情况，通常设置一个非常小的正数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 真实标签的独热编码（假设真实标签是类别 0）\n",
    "true_label = np.array([1, 0, 0])\n",
    "\n",
    "# 定义预测概率，p0 表示类别 0 的预测概率\n",
    "p0 = np.linspace(0.01, 0.99, 100)  # 避免 log(0) 的问题，取值从 0.01 到 0.99\n",
    "p1 = (1 - p0) / 2\n",
    "p2 = (1 - p0) / 2\n",
    "\n",
    "# 计算交叉熵误差\n",
    "cross_entropy_loss = - (true_label[0] * np.log(p0) + true_label[1] * np.log(p1) + true_label[2] * np.log(p2))\n",
    "\n",
    "# 绘制交叉熵误差曲线\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(p0, cross_entropy_loss, label='Cross Entropy Loss', color='b')\n",
    "plt.xlabel('Predicted Probability of True Class (p0)')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.title('Cross Entropy Loss for Multi-class Classification')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# 添加注释来解释真实标签和预测值\n",
    "plt.axvline(x=1.0, color='g', linestyle='--', label='True Label (Ideal Prediction)')\n",
    "plt.text(0.5, 2, 'Lower Loss when Prediction is Close to True Label', fontsize=10, color='red')\n",
    "plt.scatter([1.0], [0.0], color='g', marker='o', label='True Label Point (p0 = 1)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 为什么不能将识别精度作为指标？\n",
    "    - 因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变为 0。![](./attachements/为什么神经网络学习中不能以识别精度作为目标.png)\n",
    "\n",
    "\n",
    "###### mini-batch版本的交叉熵误差怎么实现？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage-01: 单个样本的交叉熵误差计算\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 根据交叉熵误差公式定义的函数（适用于单个样本）\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        # 主要目的是为了统一处理单个样本和批量数据：1维转2维 [0.8, 0.1, 0.1] ==> [[0.8, 0.1, 0.1]]\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    return -np.sum(t * np.log(y + 1e-7))\n",
    "\n",
    "# 示例：预测概率和真实标签\n",
    "y = np.array([0.8, 0.1, 0.1])  # 预测概率\n",
    "t = np.array([1, 0, 0])  # 真实标签（独热编码）\n",
    "\n",
    "# 计算交叉熵误差\n",
    "loss = cross_entropy_error(y, t)\n",
    "print(\"Cross Entropy Loss:\", loss)\n",
    "\n",
    "\n",
    "# Stage-02: mini-batch版本的交叉熵误差实现，其实mini-batch就是多个样本一起处理\n",
    "def cross_entropy_error_batch(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "\n",
    "# 示例：mini-batch 的预测概率和真实标签\n",
    "y_batch = np.array([[0.8, 0.1, 0.1],\n",
    "                    [0.2, 0.7, 0.1],\n",
    "                    [0.1, 0.3, 0.6]])  # 预测概率（3 个样本）\n",
    "t_batch = np.array([[1, 0, 0],\n",
    "                    [0, 1, 0],\n",
    "                    [0, 0, 1]])  # 真实标签（独热编码，3 个样本）\n",
    "\n",
    "# 计算 mini-batch 的交叉熵误差\n",
    "loss_batch = cross_entropy_error_batch(y_batch, t_batch)\n",
    "print(\"Cross Entropy Loss (mini-batch):\", loss_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 四、数值微分是什么？\n",
    "\n",
    "###### 中心差分为什么比前向差分好？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 定义函数及其导数\n",
    "def f(x):\n",
    "    return np.log(x + 2)  # Example function: y = log(x + 2)\n",
    "\n",
    "def df(x):\n",
    "    return 1 / (x + 2)  # True derivative of the function\n",
    "\n",
    "# 设置绘图参数\n",
    "x = 1  # Point of interest\n",
    "h = 1  # Step size\n",
    "\n",
    "x_vals = np.linspace(-1.5, 3, 400)  # Define range of x values\n",
    "\n",
    "# 计算函数值\n",
    "y_vals = f(x_vals)\n",
    "\n",
    "# 创建图形\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 绘制函数曲线\n",
    "plt.plot(x_vals, y_vals, label='$y=f(x)$', color='black', linewidth=2)\n",
    "\n",
    "# 绘制真实导数的切线\n",
    "slope_real = df(x)\n",
    "y_tangent_real = f(x) + slope_real * (x_vals - x)\n",
    "plt.plot(x_vals, y_tangent_real, label='True Tangent', color='gray', linestyle='-', linewidth=2)\n",
    "\n",
    "# 绘制前向差分直线（通过点 x 和 x+h）\n",
    "slope_forward = (f(x + h) - f(x)) / h\n",
    "intercept_forward = f(x) - slope_forward * x\n",
    "plt.plot(x_vals, slope_forward * x_vals + intercept_forward, label='Forward Difference (through x and x+h)', color='blue', linestyle='--', linewidth=2)\n",
    "\n",
    "# 绘制中心差分直线（通过点 x-h 和 x+h）\n",
    "slope_central = (f(x + h) - f(x - h)) / (2 * h)\n",
    "intercept_central = (f(x + h) - slope_central * (x + h) + f(x - h) - slope_central * (x - h)) / 2\n",
    "plt.plot(x_vals, slope_central * x_vals + intercept_central, label='Central Difference (through x-h and x+h)', color='red', linestyle='-.', linewidth=2)\n",
    "\n",
    "# 标注 x, x+h, 和 x-h 的垂直线\n",
    "plt.axvline(x, color='gray', linestyle='--', linewidth=1)\n",
    "plt.axvline(x + h, color='gray', linestyle='--', linewidth=1)\n",
    "plt.axvline(x - h, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "# 标注点 x, x+h, 和 x-h\n",
    "plt.text(x, f(x) - 0.5, '$x$', horizontalalignment='right', color='black')\n",
    "plt.text(x + h, f(x + h) - 0.5, '$x+h$', horizontalalignment='left', color='black')\n",
    "plt.text(x - h, f(x - h) - 0.5, '$x-h$', horizontalalignment='left', color='black')\n",
    "plt.text(x + h/2, f(x) + 0.7, f'$h={h}$', horizontalalignment='center', color='green')\n",
    "\n",
    "# 标注点 (x, f(x))\n",
    "plt.scatter([x], [f(x)], color='black', zorder=5)\n",
    "\n",
    "# 图例和标签\n",
    "plt.legend()\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('Comparison of Derivative, Forward Difference, and Central Difference')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 数值微分是什么？\n",
    "> 利用微小的差分求导数的过程被称为`数值微分`numerical differentiation。通过数学式的推导求导数的过程被称为解析解比如$y=x^2$在$x=2$处的导数是$y’ = 2x = 4$。\n",
    "\n",
    "###### 偏导数是什么？怎么求？\n",
    "> 有多个变量的函数的导数就被称为偏导数。偏指的是一个自变量一个导数！\n",
    "> 偏导数求解就是：将多个变量中的某一个变量定为目标变量，并将其他变量固定为某个值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中心差分求导数\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "\n",
    "\n",
    "# 求偏导数的过程：求函数 f(x0, x1) = x0^2 + x1^2 在点x0 = 3 x1 = 4处的偏导数\n",
    "def function_tmp1(x0):\n",
    "    return x0 * x0 + 4.0**2.0\n",
    "\n",
    "def function_tmp2(x1):\n",
    "    return 3.0**2.0 + x1*x1\n",
    "\n",
    "print(numerical_diff(function_tmp1, 3.0))\n",
    "print(numerical_diff(function_tmp2, 4.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 五、梯度\n",
    "\n",
    "###### 什么是梯度？\n",
    "> 梯度就是多变量函数中由全部变量的偏导数汇总而成的向量！比如上面$x_0=3,x_1=4时求偏导数$如果一次性求出就是$(\\frac{\\partial f}{\\partial x_0} ,\\frac{\\partial f}{\\partial x_1})$这就是梯度！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 定义二元函数\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "    # 或者return np.sum(x**2)\n",
    "\n",
    "\n",
    "# 定义求梯度的函数\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # 生成与x形状相同的数组，好保存每一个偏导数\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # 计算f(x+h)\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        # 计算f(x-h)\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val \n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "numerical_gradient(function_2, np.array([3.0, 4.0]))\n",
    "# numerical_gradient(function_2, np.array([0.0, 2.0]))\n",
    "# numerical_gradient(function_2, np.array([3.0, 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 什么是鞍点？\n",
    "> 鞍点是从某个方向上看是极大值，从另一个方向上看则是极小值的点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# 定义鞍点函数\n",
    "def saddle_point(x, y):\n",
    "    return x**2 - y**2  # 示例鞍点函数：z = x^2 - y^2\n",
    "\n",
    "# 设置 x 和 y 的取值范围\n",
    "x = np.linspace(-2, 2, 100)\n",
    "y = np.linspace(-2, 2, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "# 计算 z 的值\n",
    "z = saddle_point(x, y)\n",
    "\n",
    "# 创建图形\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# 绘制鞍点的 3D 曲面\n",
    "ax.plot_surface(x, y, z, cmap='viridis', alpha=0.8)\n",
    "\n",
    "# 标注图形\n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_zlabel('Z axis')\n",
    "ax.set_title('Saddle Point Visualization: $z = x^2 - y^2$')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 怎么实现梯度下降法GD？\n",
    "\n",
    "很简单就是\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "x_{0}=x_{0}-\\eta \\frac{\\partial f}{\\partial x_{0}} \\\\\n",
    "x_{1}=x_{1}-\\eta \\frac{\\partial f}{\\partial x_{1}}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度下降法实现：原来就是【随便选一个初始点】--> 迭代step_num步 -->每一步都求一下该点的偏导数，然后让原来的点按照梯度的变化方向变化\n",
    "\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "# 我们试下用梯度法求f(x0 + x1) = x0^2 + x1^2的最小值\n",
    "def function_2(x):\n",
    "    return np.sum(x**2)\n",
    "\n",
    "init_x = np.array([-3, 4], dtype=np.float64) # 随便设置的起始点\n",
    "# init_x = np.array([-3.0, 4.0], dtype=np.float64)\n",
    "gradient_descent(function_2, init_x, lr=0.1, step_num=101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 定义二元函数\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "# 定义求梯度的函数\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4  # 0.0001\n",
    "    grad = np.zeros_like(x)  # 生成与x形状相同的数组，好保存每一个偏导数\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # 计算f(x+h)\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        # 计算f(x-h)\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)  # 修正为中心差分的正确公式\n",
    "        x[idx] = tmp_val\n",
    "\n",
    "    return grad\n",
    "\n",
    "# 梯度下降法实现\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []  # 记录每一步的 x 值\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append(x.copy())  # 保存当前点\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x, np.array(x_history)\n",
    "\n",
    "# 我们试下用梯度法求 f(x0 + x1) = x0^2 + x1^2 的最小值\n",
    "init_x = np.array([-3.0, 4.0])  # 随便设置的初始点\n",
    "lr = 0.1 # 可以改大改小玩儿玩儿\n",
    "step_num = 101\n",
    "\n",
    "# 执行梯度下降\n",
    "final_x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\n",
    "\n",
    "# 绘制梯度下降过程的图形\n",
    "x0_vals = x_history[:, 0]\n",
    "x1_vals = x_history[:, 1]\n",
    "\n",
    "# 创建等高线图\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = function_2(np.array([x, y]))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# 绘制等高线\n",
    "contour = plt.contour(x, y, z, levels=30, cmap='viridis')\n",
    "plt.clabel(contour)\n",
    "\n",
    "# 绘制梯度下降路径\n",
    "plt.plot(x0_vals, x1_vals, 'o-', color='red', markersize=4, label='Gradient Descent Path')\n",
    "plt.scatter(0, 0, color='blue', marker='*', s=100, label='Minimum')\n",
    "\n",
    "# 标注\n",
    "plt.xlabel('$x_0$')\n",
    "plt.ylabel('$x_1$')\n",
    "plt.title('Gradient Descent on $f(x_0, x_1) = x_0^2 + x_1^2$')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 什么是超参数？跟神经网络的参数不一样吗？\n",
    "    - 超参数：比如学习率，是人工设定的参数。\n",
    "    - 神经网络的参数：比如权重偏置，是通过训练数据和学习算法自动获得的。\n",
    "\n",
    "###### 什么是epoch?\n",
    "> 所有的训练数据都被使用过一次了！比如10000笔训练数据，mini-batch=100那重复GD100次就是一个epoch！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 手动实现一个mini-batch的学习算法！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Story: mini-batch的学习：从训练数据中随机选择一部分数据 ==》以这部分数据（一个mini-batch）为对象，使用GD法跟更新参数\n",
    "\n",
    "# Stage-01: 定义一个2层的nn\n",
    "\n",
    "# Stage-02: 获取mini-batch数据进入到NN中训\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 定义求梯度的函数\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4  # 0.0001\n",
    "    grad = np.zeros_like(x)  # 生成与x形状相同的数组，用于保存每一个偏导数\n",
    "\n",
    "    # 使用 np.nditer 遍历数组的每个元素\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # 计算 f(x+h)\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        # 计算 f(x-h)\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        # 计算梯度\n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "\n",
    "        # 还原 x 的值\n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "\n",
    "    return grad\n",
    "\n",
    "class TwolayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01) -> None:\n",
    "        # 初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        c = np.max(x)\n",
    "        exp_x = np.exp(x - c)  # 数值稳定性\n",
    "        return exp_x / np.sum(exp_x)\n",
    "    \n",
    "    def cross_entropy_error_batch(self, y, t):\n",
    "        if y.ndim == 1:\n",
    "            t = t.reshape(1, t.size)\n",
    "            y = y.reshape(1, y.size)\n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(t * np.log(y + 1e-7)) / batch_size \n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = self.sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = self.softmax(a2)\n",
    "\n",
    "        return y\n",
    "    \n",
    "\n",
    "    # x:输入数据， t:监督数据\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.cross_entropy_error_batch(y, t)\n",
    "    \n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "\n",
    "        # 如果 t 是 one-hot 编码，则取 argmax\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "    \n",
    "    # x:输入数据， t:监督数据\n",
    "    def numerical_gradient(self, x, t):\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(lambda W: self.loss(x, t), self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(lambda b: self.loss(x, t), self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(lambda W: self.loss(x, t), self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(lambda b: self.loss(x, t), self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "net = TwolayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "net.params['W1'].shape # (784, 100)\n",
    "net.params['b1'].shape # (100,)\n",
    "net.params['W2'].shape # (100, 10)\n",
    "net.params['b2'].shape # (10,)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现mini-batch学习\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    # 使用 sklearn 的内置函数加载 MNIST 数据集\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "    x = mnist.data / 255.0  # 归一化处理\n",
    "    t = mnist.target.astype(int)\n",
    "    x_train, x_test, t_train, t_test = train_test_split(x, t, test_size=0.2, random_state=42)\n",
    "    return x_train, t_train, x_test, t_test\n",
    "\n",
    "train_lost_list = []\n",
    "x_train, t_train, x_test, t_test  = get_data()\n",
    "\n",
    "# 超参数\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwolayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 获取mini-batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = np.eye(10)[t_train[batch_mask]]  # 将标签转换为one-hot形式\n",
    "\n",
    "    # 计算梯度\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    # 更新参数\n",
    "    for key in ('W1', 'W2', 'b1', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "\n",
    "    # 记录学习过程\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_lost_list.append(loss)\n",
    "\n",
    "\n",
    "train_lost_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.09785714285714285, 0.09971428571428571\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m10\u001b[39m)[t_train[batch_mask]]  \u001b[38;5;66;03m# 将标签转换为one-hot形式\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 计算梯度\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 更新参数\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[0;32mIn[20], line 99\u001b[0m, in \u001b[0;36mTwolayerNet.numerical_gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     96\u001b[0m loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t)\n\u001b[1;32m     98\u001b[0m grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 99\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(\u001b[38;5;28;01mlambda\u001b[39;00m b: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    101\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(\u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[20], line 26\u001b[0m, in \u001b[0;36mnumerical_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 计算 f(x-h)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m x[idx] \u001b[38;5;241m=\u001b[39m tmp_val \u001b[38;5;241m-\u001b[39m h\n\u001b[0;32m---> 26\u001b[0m fxh2 \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 计算梯度\u001b[39;00m\n\u001b[1;32m     29\u001b[0m grad[idx] \u001b[38;5;241m=\u001b[39m (fxh1 \u001b[38;5;241m-\u001b[39m fxh2) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m h)\n",
      "Cell \u001b[0;32mIn[20], line 99\u001b[0m, in \u001b[0;36mTwolayerNet.numerical_gradient.<locals>.<lambda>\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m     96\u001b[0m loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t)\n\u001b[1;32m     98\u001b[0m grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 99\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(\u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    100\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(\u001b[38;5;28;01mlambda\u001b[39;00m b: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    101\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(\u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[20], line 77\u001b[0m, in \u001b[0;36mTwolayerNet.loss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m---> 77\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_entropy_error_batch(y, t)\n",
      "Cell \u001b[0;32mIn[20], line 67\u001b[0m, in \u001b[0;36mTwolayerNet.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m W1, W2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     65\u001b[0m b1, b2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 67\u001b[0m a1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b1\n\u001b[1;32m     68\u001b[0m z1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(a1)\n\u001b[1;32m     69\u001b[0m a2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(z1, W2) \u001b[38;5;241m+\u001b[39m b2\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 优化：每经过一个epoch，记录下训练数据和测试数据的识别精度\n",
    "\n",
    "# 实现mini-batch学习\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    # 使用 sklearn 的内置函数加载 MNIST 数据集\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "    x = mnist.data / 255.0  # 归一化处理\n",
    "    t = mnist.target.astype(int)\n",
    "    x_train, x_test, t_train, t_test = train_test_split(x, t, test_size=0.2, random_state=42)\n",
    "    return x_train, t_train, x_test, t_test\n",
    "\n",
    "train_lost_list = []\n",
    "x_train, t_train, x_test, t_test  = get_data()\n",
    "\n",
    "# 看epoch的精度\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "# 平均每个 epoch 的重复次数\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "# 超参数\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwolayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 获取mini-batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = np.eye(10)[t_train[batch_mask]]  # 将标签转换为one-hot形式\n",
    "\n",
    "    # 计算梯度\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    # 更新参数\n",
    "    for key in ('W1', 'W2', 'b1', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "\n",
    "    # 记录学习过程\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_lost_list.append(loss)\n",
    "\n",
    "    # 计算每个 epoch 的识别精度\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "\n",
    "train_lost_list\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1NLP-1CODE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
