{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一、自然语言处理基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1、自然语言处理的根本任务是什么？\n",
    "\n",
    "让计算机理解我们人类的语言！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2、计算机是怎么理解人类的自然语言的？\n",
    "\n",
    "自然语言 ==》 单词  ==》 数值向量 ==》上下文理解 ==》具体任务模型参数学习！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3、什么是软语言？什么是硬语言？\n",
    "\n",
    "硬语言：编程语言就是，机械的缺乏活力的语言，哈哈。\n",
    "软语言：不同语境相同表述不同含义，新词持续诞生"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.4、自然语言处理技术在当今的时代有什么成熟的应用？有什么前沿的应用方向？\n",
    "\n",
    "> 自然语言处理技术在当今的成熟应用包括：\n",
    "\n",
    "1. **智能助手**：如Siri、Alexa，用于语音识别和对话系统。\n",
    "2. **机器翻译**：如Google翻译，实现多语言即时翻译。\n",
    "3. **文本生成**：如ChatGPT，生成自然语言回答或内容。\n",
    "4. **情感分析**：用于社交媒体、客户反馈等领域分析情绪。\n",
    "5. **信息检索**：如搜索引擎，提升信息查询的精准度。\n",
    "\n",
    "这些应用改变了人机交互、全球通信和数据处理的效率。\n",
    "\n",
    "> 自然语言处理技术在未来的探索方向包括：\n",
    "\n",
    "1. **医疗诊断与健康助手**：通过分析病历、对话和研究文献，辅助医生诊断或为患者提供个性化健康建议。\n",
    "  \n",
    "2. **法律与合同审查**：自动分析和生成法律文书、合同审查，减少人工工作量。\n",
    "\n",
    "3. **金融智能分析**：实时解读市场新闻、分析金融报告，自动化投资决策或风险管理。\n",
    "\n",
    "4. **自动化科研助理**：帮助科学家整理、归纳海量学术论文或生成新研究思路。\n",
    "\n",
    "5. **虚拟教育助理**：智能答疑、个性化教学，提供更沉浸式的学习体验。\n",
    "\n",
    "这些方向将进一步拓展NLP在专业领域的深度应用。\n",
    "\n",
    "> 开个脑洞\n",
    "\n",
    "\n",
    "1. **脑机接口（BCI）与语言理解**：通过直接解码大脑信号，将思维转化为语言，实现“思维交互”，无需物理输入设备。\n",
    "\n",
    "2. **情感增强虚拟现实（VR/AR）**：结合NLP和情感计算，创造能感知并回应用户情绪的虚拟角色，打造更加自然的沉浸式体验。\n",
    "\n",
    "3. **AI驱动的社会治理**：通过分析社会舆情、政策文本，提供智能化的公共决策建议，促进社会问题的前瞻性解决。\n",
    "\n",
    "4. **跨物种交流**：运用NLP技术尝试解读动物或外星生命的信号，探索与非人类生命形式的“语言”互动。\n",
    "\n",
    "5. **完全自主的虚拟人格**：创造拥有自主学习能力、情感认知与社会行为的虚拟人类，推动人与虚拟存在的深度共生。\n",
    "\n",
    "这些方向更具未来感，将重新定义人机交互的边界。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.5、为什么研究NLP一开始人类要先研究单词呢？\n",
    "\n",
    "\n",
    "我们的语言是由文字构成的，而语言的含义是由单词构成的。换句话说，*单词是含义的最小单位。*因此，为了让计算机理解自然语言，让它理解单词含义可以说是最重要的事情了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 二、同义词词典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1、什么是WordNet？有什么用？有什么问题？\n",
    "\n",
    "> 是什么？\n",
    "\n",
    "- 本质：根据各单词的含义，基于上位 - 下位关系形成的图，目的是根据这个人工定义的网络来计算单词之间的相似度！\n",
    "![](./attachements/WordNet构建示意图.png)\n",
    "\n",
    "- NLTK模块中有一个WordNet是是普林斯顿大学于 1985 年开始开发的同义词词典，超过20万个单词\n",
    "> 有什么用？\n",
    "\n",
    "- 根据WordNet计算单词之间的相似度！\n",
    "\n",
    "> 有什么问题？\n",
    "- 人力成本高 + 无法表示单词之间的微妙变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2、怎么从海量的文本数据中自动提取单词含义，以解放人们从人工关联单词的痛苦工作？\n",
    "\n",
    "1. **基于计数的方法**：通过统计单词在不同上下文中的共现频率（如TF-IDF、共现矩阵），自动推测单词含义。它利用词与其他词的共现信息生成语义关系，无需手动定义词义。\n",
    "\n",
    "2. **基于神经网络的推理方法**：利用深度学习模型（如Word2Vec、BERT），通过大规模语料训练网络，自动学习词的向量表示。模型根据上下文推理出单词的含义，替代了人工标注和关联词义的工作。\n",
    "\n",
    "这两种方法通过自动化的数据驱动方式，快速且高效地构建单词的语义关联，避免人工处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 三、基于计数的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1、什么是语料库？\n",
    "\n",
    "语料库（corpus）: 语料库就是一个**由大量人类写的文本组成的数据库**，它记录了人们使用语言的方式，包括词的使用和句子的结构。通过分析这些文本，机器可以从中学到语言的规则和词的含义，而不需要人工标注。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2、基于计数的方法整体实现思路是什么？\n",
    "\n",
    "使用语料库，计算上下文中的单词数量，将它们转化 PPMI 矩阵，再基于 SVD 降维获得好的单词向量。这就是单词的分布式表示，每个单词表示为固定长度的密集向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4、语料库的预处理需要做什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def preprocess(text: str):\n",
    "    text = text.lower()\n",
    "    # text = re.split('(\\W+)?', text) \n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "    \n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "print(corpus)\n",
    "print(word_to_id)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5、单词的分布式表示是什么意思？\n",
    "\n",
    "单词的分布式表示就等同于颜色的RGB表示：本质是将单词表示为固定长度的密集向量表示！\n",
    "\n",
    "- RGB(220, 1, 1)一看就知道是红色系的哈哈\n",
    "- 密集向量：就是组成向量的大多数元素都非零"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6、什么是分布式假设？什么是单词的上下文？\n",
    "\n",
    "- 分布式假设：就是一个想法\"某个单词的含义由它周围的单词形成\"\n",
    "- 单词的上下文：滑动窗口slid_window就是上下文的具象算法表现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.7、什么是共现矩阵？\n",
    "\n",
    "- 共现矩阵：就是基于计数的方法的，这是一个东西！**本质是计算每个单词的上下文所包含的单词的频数！**\n",
    "\n",
    "![](./attachements/单词you的上下文.png)\n",
    "![](./attachements/单词you的上下文中包含的单词频数.png)\n",
    "![](./attachements/单词say的上下文频数.png)\n",
    "![](./attachements/汇总各单词上下文频数.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求共线矩阵\n",
    "\n",
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "            \n",
    "            # 检查左侧窗口\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "            \n",
    "            # 检查右侧窗口\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "    \n",
    "    return co_matrix  # 确保在整个循环完成后才返回\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.8、怎么度量两个向量间的相似度？\n",
    "\n",
    "> 余弦相似度直观地表示了“两个向量在多大程度上指向同一方向”。两个向量完全指向相同的方向时，余弦相似度为 1；完全指向相反的方向时，余弦相似度为 −1。\n",
    "\n",
    "$$\n",
    "\\operatorname{similarity}(\\boldsymbol{x}, \\boldsymbol{y})=\\frac{\\boldsymbol{x} \\cdot \\boldsymbol{y}}{\\|\\boldsymbol{x}\\|\\|\\boldsymbol{y}\\|}=\\frac{x_{1} y_{1}+\\cdots+x_{n} y_{n}}{\\sqrt{x_{1}^{2}+\\cdots+x_{n}^{2}} \\sqrt{y_{1}^{2}+\\cdots+y_{n}^{2}}}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cos_similarity(x, y):\n",
    "#     nx = x / np.sqrt(np.sum(x**2)) # x 的正规化\n",
    "#     ny = y / np.sqrt(np.sum(y**2)) # y 的正规化\n",
    "#     return np.dot(nx, ny)\n",
    "\n",
    "# 一般放置除数为0我们就加个微小量1e-8\n",
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "0.7071067691154799\n"
     ]
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "c0 = C[word_to_id['you']] # you 的单词向量\n",
    "c1 = C[word_to_id['i']] # i 的单词向量\n",
    "\n",
    "print(C)\n",
    "print(cos_similarity(c0, c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 四、基于计数的方法的问题驱动演进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1、基于计数的方法中根据余弦相似度计算向量间的相似度有什么问题？\n",
    "\n",
    "> 答：比如，我们来考虑某个语料库中 the 和 car 共现的情况。在这种情况下，我们会看到很多“...the car...”这样的短语。因此，它们的共现次数将会很大。另外，car 和 drive 也明显有很强的相关性。但是，如果只看单词的出现次数，那么与 drive 相比，the 和 car 的相关性更强。这意味着，仅仅因为 the 是个常用词，它就被认为与 car 有很强的相关性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    # ❶ 取出查询词\n",
    "    if query not in word_to_id:\n",
    "        print('%s is not found' % query)\n",
    "        return\n",
    "    \n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    # ❷ 计算余弦相似度\n",
    "    vocab_size = len(id_to_word)\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    # ❸ 基于余弦相似度，按降序输出值\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " goodbye: 0.7071067691154799\n",
      " i: 0.7071067691154799\n",
      " hello: 0.7071067691154799\n",
      " say: 0.0\n",
      " and: 0.0\n"
     ]
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "most_similar('you', word_to_id, id_to_word, C, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 你看上面的相似单词的排序代码实现结果，i我可以理解因为都是人称代词，那为毛跟goodbye还有hello都近呢？ ==》 一个可能的原因是，这里的语料库太小了！！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2、PPMI（点互信息）矩阵怎么求？又带来了什么问题？\n",
    "\n",
    "$$\n",
    "\\operatorname{PMI}(x, y)=\\log _{2} \\frac{P(x, y)}{P(x) P(y)}=\\log _{2} \\frac{\\frac{\\boldsymbol{C}(x, y)}{N}}{\\frac{\\boldsymbol{C}(x)}{N} \\frac{\\boldsymbol{C}(y)}{N}}=\\log _{2} \\frac{\\boldsymbol{C}(x, y) \\cdot N}{\\boldsymbol{C}(x) \\boldsymbol{C}(y)}\n",
    "$$\n",
    "\n",
    "> 正的点互信息：因为点互信息当两个单词的共现次数为 0 时，$log_{2}0 = -\\infty $\n",
    "\n",
    "$$\n",
    "PPMI(x, y) = max(0, PMI(x, y)) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppmi(C, verbose=False, eps=1e-8):\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
    "            M[i, j] = max(0, pmi)\n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total//100+1) == 0:\n",
    "                    print('%.1f%% done' % (100*cnt/total))\n",
    "    return M    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance matrix\n",
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "--------------------------------------------------\n",
      "PPMI\n",
      "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.807 0.    0.    0.    0.    2.807]\n",
      " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "W = ppmi(C)\n",
    "np.set_printoptions(precision=3) # 有效位数为 3位\n",
    "print('covariance matrix')\n",
    "print(C)\n",
    "print('-'*50)\n",
    "print('PPMI')\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "问题：\n",
    "- 随着语料库的词汇量增加，各个单词向量的维度会高的离谱\n",
    "- PPMI矩阵是稀疏矩阵，为0的元素很多，也就意味着每个真正的”信息元素“话语权小，容易受噪声影响，稳健性差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3、怎么从稀疏向量中找出重要的轴，用更少的维度对其进行重新表示？\n",
    "\n",
    "![](./attachements/降维示意图-找到数据分布广的轴.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4、为什么基于SVD的奇异值分解降维有用？\n",
    "\n",
    "![](./attachements/SVD分解-原矩阵.png)\n",
    "![](./attachements/SVD分解-取密集矩阵.png)\n",
    "\n",
    "矩阵 S 的奇异值小，对应的基轴的重要性低，因此，可以通过去除矩阵 U 中的多余的列向量来近似原始矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0]\n",
      "[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      "[ 3.409e-01 -1.110e-16 -1.205e-01 -4.163e-16 -9.323e-01 -1.110e-16\n",
      " -2.426e-17]\n",
      "[ 3.409e-01 -1.110e-16]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(id_to_word)\n",
    "C = create_co_matrix(corpus, vocab_size, window_size=1)\n",
    "W = ppmi(C)\n",
    "# SVD\n",
    "U, S, V = np.linalg.svd(W)\n",
    "\n",
    "print(C[0]) # 共现矩阵\n",
    "print(W[0]) # PPMI 矩阵\n",
    "print(U[0]) # SVD\n",
    "print(U[0, :2]) # 降维到二维向量，取出前两个元素即可。\n",
    "\n",
    "\n",
    "for word, word_id in word_to_id.items():\n",
    "    plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
    "\n",
    "plt.scatter(U[:,0], U[:,1], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goodbye 和 hello、you 和 i 位置接近，这是比较符合我们的直觉的。但是，因为我们使用的语料库很小，有些结果就比较微妙。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.5、Truncated SVD是为了解决什么问题的？\n",
    "\n",
    "- 因为SVD的计算复杂度是$O(n^3)$, sklearn.utils.extmath import randomized_svd就是一种Truncated SVD  == 》 仅对奇异值较大的部分进行计算，计算速度比常规的 SVD 快\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.6、换PTB语料库\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/xiaomengli/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 100676\n",
      "Vocabulary size: 11387\n"
     ]
    }
   ],
   "source": [
    "# NLTK中载入不是符合我们上面的corpub word_2_id id_2_word 转一下\n",
    "\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# 下载并加载 PTB 语料库\n",
    "nltk.download('treebank')\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "# 创建目录用于保存数据\n",
    "dataset_dir = './data'  # 你可以自定义路径\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "vocab_file = os.path.join(dataset_dir, 'ptb.vocab.pkl')\n",
    "\n",
    "def load_vocab():\n",
    "    if os.path.exists(vocab_file):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            word_to_id, id_to_word = pickle.load(f)\n",
    "        return word_to_id, id_to_word\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "\n",
    "    # 从 PTB 语料库获取所有单词\n",
    "    words = [word.lower() for sentence in treebank.sents() for word in sentence]\n",
    "\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            word_id = len(word_to_id)\n",
    "            word_to_id[word] = word_id\n",
    "            id_to_word[word_id] = word\n",
    "\n",
    "    with open(vocab_file, 'wb') as f:\n",
    "        pickle.dump((word_to_id, id_to_word), f)\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def load_data():\n",
    "    word_to_id, id_to_word = load_vocab()\n",
    "\n",
    "    # 创建语料\n",
    "    words = [word.lower() for sentence in treebank.sents() for word in sentence]\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    corpus, word_to_id, id_to_word = load_data()\n",
    "    print(\"Corpus size:\", len(corpus))\n",
    "    print(\"Vocabulary size:\", len(word_to_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting co-occurrence ...\n",
      "calculating PPMI ...\n",
      "1.0% done\n",
      "2.0% done\n",
      "3.0% done\n",
      "4.0% done\n",
      "5.0% done\n",
      "6.0% done\n",
      "7.0% done\n",
      "8.0% done\n",
      "9.0% done\n",
      "10.0% done\n",
      "11.0% done\n",
      "12.0% done\n",
      "13.0% done\n",
      "14.0% done\n",
      "15.0% done\n",
      "16.0% done\n",
      "17.0% done\n",
      "18.0% done\n",
      "19.0% done\n",
      "20.0% done\n",
      "21.0% done\n",
      "22.0% done\n",
      "23.0% done\n",
      "24.0% done\n",
      "25.0% done\n",
      "26.0% done\n",
      "27.0% done\n",
      "28.0% done\n",
      "29.0% done\n",
      "30.0% done\n",
      "31.0% done\n",
      "32.0% done\n",
      "33.0% done\n",
      "34.0% done\n",
      "35.0% done\n",
      "36.0% done\n",
      "37.0% done\n",
      "38.0% done\n",
      "39.0% done\n",
      "40.0% done\n",
      "41.0% done\n",
      "42.0% done\n",
      "43.0% done\n",
      "44.0% done\n",
      "45.0% done\n",
      "46.0% done\n",
      "47.0% done\n",
      "48.0% done\n",
      "49.0% done\n",
      "50.0% done\n",
      "51.0% done\n",
      "52.0% done\n",
      "53.0% done\n",
      "54.0% done\n",
      "55.0% done\n",
      "56.0% done\n",
      "57.0% done\n",
      "58.0% done\n",
      "59.0% done\n",
      "60.0% done\n",
      "61.0% done\n",
      "62.0% done\n",
      "63.0% done\n",
      "64.0% done\n",
      "65.0% done\n",
      "66.0% done\n",
      "67.0% done\n",
      "68.0% done\n",
      "69.0% done\n",
      "70.0% done\n",
      "71.0% done\n",
      "72.0% done\n",
      "73.0% done\n",
      "74.0% done\n",
      "75.0% done\n",
      "76.0% done\n",
      "77.0% done\n",
      "78.0% done\n",
      "79.0% done\n",
      "80.0% done\n",
      "81.0% done\n",
      "82.0% done\n",
      "83.0% done\n",
      "84.0% done\n",
      "85.0% done\n",
      "86.0% done\n",
      "87.0% done\n",
      "88.0% done\n",
      "89.0% done\n",
      "90.0% done\n",
      "91.0% done\n",
      "92.0% done\n",
      "93.0% done\n",
      "94.0% done\n",
      "95.0% done\n",
      "96.0% done\n",
      "97.0% done\n",
      "98.0% done\n",
      "99.0% done\n",
      "calculating SVD ...\n",
      "\n",
      "[query] you\n",
      " i: 0.7197004556655884\n",
      " we: 0.7132217884063721\n",
      " stuff: 0.6975018978118896\n",
      " flush: 0.6478176116943359\n",
      " your: 0.6459861397743225\n",
      "\n",
      "[query] year\n",
      " last: 0.849858820438385\n",
      " week: 0.7580245137214661\n",
      " early: 0.7460704445838928\n",
      " ending: 0.7216872572898865\n",
      " march: 0.7164829969406128\n",
      "\n",
      "[query] car\n",
      " kidnapper: 0.4773333966732025\n",
      " 1982: 0.4487912058830261\n",
      " vs.: 0.4252346158027649\n",
      " imsai: 0.403319776058197\n",
      " affirmative: 0.39991459250450134\n",
      "toyota is not found\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "wordvec_size = 100\n",
    "# corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "print('counting co-occurrence ...')\n",
    "C = create_co_matrix(corpus, vocab_size, window_size)\n",
    "print('calculating PPMI ...')\n",
    "W = ppmi(C, verbose=True)\n",
    "print('calculating SVD ...')\n",
    "try:\n",
    "    # truncated SVD (fast!)\n",
    "    from sklearn.utils.extmath import randomized_svd\n",
    "    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5,\n",
    "    random_state=None)\n",
    "except ImportError:\n",
    "    # SVD (slow)\n",
    "    U, S, V = np.linalg.svd(W)\n",
    "\n",
    "word_vecs = U[:, :wordvec_size]\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1NLP-1CODE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
